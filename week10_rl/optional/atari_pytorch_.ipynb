{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:02:07.057065Z",
     "start_time": "2018-12-21T09:02:06.738576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: ../xvfb: No such file or directory\n",
      "env: DISPLAY=:1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "#If you are running on a server, launch xvfb to record game videos\n",
    "#Please make sure you have xvfb installed\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kung-Fu, recurrent style\n",
    "\n",
    "In this notebook we'll once again train RL agent for for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), this time using recurrent neural networks.\n",
    "\n",
    "![http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg](http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:02:08.432975Z",
     "start_time": "2018-12-21T09:02:08.287643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop = lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:02:09.704844Z",
     "start_time": "2018-12-21T09:02:09.365680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42,42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:04:54.103430Z",
     "start_time": "2018-12-21T09:04:53.980036Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:43:40.840147Z",
     "start_time": "2018-12-21T09:43:40.835681Z"
    }
   },
   "outputs": [],
   "source": [
    "?nn.LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:48:55.975192Z",
     "start_time": "2018-12-21T09:48:55.967008Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "        rnn_input=F.relu(self.hid(\n",
    "            self.flatten(F.elu(self.conv2(F.elu(self.conv1(F.elu(self.conv0(obs_t)))))))\n",
    "        ))\n",
    "        \n",
    "        new_state = self.rnn(rnn_input,prev_state)\n",
    "        logits = self.logits(new_state[0])\n",
    "        state_value = self.state_value(new_state[0])\n",
    "        \n",
    "        return new_state, (logits, state_value)\n",
    "    \n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return (Variable(torch.zeros((batch_size, 128))),\n",
    "                Variable(torch.zeros((batch_size, 128))))\n",
    "    \n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "    \n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is not Variable \"\"\"\n",
    "        obs_t = Variable(torch.FloatTensor(np.array(obs_t)))\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:48:59.802155Z",
     "start_time": "2018-12-21T09:48:59.795641Z"
    }
   },
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:49:03.959388Z",
     "start_time": "2018-12-21T09:49:03.955739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRecurrentAgent(\n",
       "  (conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (flatten): Flatten()\n",
       "  (hid): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (rnn): LSTMCell(128, 128)\n",
       "  (logits): Linear(in_features=128, out_features=14, bias=True)\n",
       "  (state_value): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:49:13.530969Z",
     "start_time": "2018-12-21T09:49:13.507067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action logits:\n",
      " tensor([[ 0.0356,  0.0662,  0.0787,  0.0082, -0.0744,  0.0592, -0.0566, -0.0029,\n",
      "          0.0758, -0.0205, -0.0769, -0.0613,  0.0083,  0.0587]])\n",
      "state values:\n",
      " tensor([[0.0188]])\n"
     ]
    }
   ],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:49:20.278600Z",
     "start_time": "2018-12-21T09:49:20.274062Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done: break\n",
    "                \n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:49:33.189779Z",
     "start_time": "2018-12-21T09:49:25.660333Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanyuhuang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/hanyuhuang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1600.0, 600.0, 1300.0]\n"
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print (rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:49:37.030537Z",
     "start_time": "2018-12-21T09:49:37.024370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./kungfu_videos/openaigym.video.0.98166.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](img2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:50:45.711913Z",
     "start_time": "2018-12-21T09:50:44.989226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:51:03.454982Z",
     "start_time": "2018-12-21T09:51:03.368398Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanyuhuang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:51:06.564912Z",
     "start_time": "2018-12-21T09:51:06.560288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions shape: (5, 10)\n",
      "Rewards shape: (5, 10)\n",
      "Mask shape: (5, 10)\n",
      "Observations shape:  (5, 10, 1, 42, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \",rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T09:56:03.611058Z",
     "start_time": "2018-12-21T09:56:03.607237Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return Variable(y_one_hot) if isinstance(y, Variable) else y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:00:56.053141Z",
     "start_time": "2018-12-24T10:00:56.043420Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # cast everything into a variable\n",
    "    states = Variable(torch.FloatTensor(np.array(states)))   # shape: [batch_size, time, c, h, w]\n",
    "    actions = Variable(torch.IntTensor(np.array(actions)))   # shape: [batch_size, time]\n",
    "    rewards = Variable(torch.FloatTensor(np.array(rewards))) # shape: [batch_size, time]\n",
    "    is_not_done = Variable(torch.FloatTensor(is_not_done.astype('float32')))  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent. \n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "    \n",
    "    logits = [] # append logit sequence here\n",
    "    state_values = [] #append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "        \n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "        \n",
    "        memory, (logits_t, values_t) = agent.step(memory,obs_t)\n",
    "        \n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "        \n",
    "        \n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "        \n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim = -1)\n",
    "    #print(logprobas_for_actions.shape)\n",
    "    \n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective. \n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    J_hat = 0 #Variable(torch.FloatTensor(0),requires_grad=True) # policy objective as in the formula for J_hat\n",
    "    \n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0 #Variable(torch.FloatTensor(0),requires_grad=True)\n",
    "    \n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "    \n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                                # current rewards\n",
    "        V_t = state_values[:, t]                           # current state values\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]          # log-probability of a_t in s_t\n",
    "        \n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "        \n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "        value_loss += ((V_t-(r_t+gamma * V_next))**2).mean()\n",
    "        \n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = cumulative_returns-V_t\n",
    "        advantage = advantage.detach()\n",
    "        \n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "        J_hat += (advantage*logprobas_for_actions[:,t]).mean()\n",
    "        \n",
    "    #regularize with entropy\n",
    "    entropy_reg = -((probas*logprobas).sum(dim=1)).mean()\n",
    "    \n",
    "    # add-up three loss components and average over time\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "           value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "    print(J_hat,J_hat.requires_grad)\n",
    "    print(entropy_reg,entropy_reg.requires_grad)\n",
    "    print(loss,loss.requires_grad)\n",
    "    \n",
    "    # Gradient descent step\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:00:56.588430Z",
     "start_time": "2018-12-24T10:00:56.480167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0322) False\n",
      "tensor(1.8839) False\n",
      "tensor(-0.0224) False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanyuhuang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-777616ab0473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrollout_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollout_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollout_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollout_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_on_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollout_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollout_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollout_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-045ad1418977>\u001b[0m in \u001b[0;36mtrain_on_rollout\u001b[0;34m(states, actions, rewards, is_not_done, prev_memory_states, gamma)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# Gradient descent step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# let's test it\n",
    "agent.train()\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions, rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import ewma\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(15000):  \n",
    "    \n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "    train_on_rollout(rollout_obs, rollout_actions, rollout_rewards, rollout_mask, memory)    \n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(ewma(np.array(rewards_history),span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
